{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\Users\\User\\Desktop\\Ai_Exercises\\intro-to-tensorflow\\tf_intro_venv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Ai_Exercises\\intro-to-tensorflow\\tf_intro_venv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m   \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pywrap_tensorflow_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[32m     78\u001b[39m \n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Ai_Exercises\\intro-to-tensorflow\\tf_intro_venv\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[39m\n\u001b[32m     37\u001b[39m _os.environ.setdefault(\u001b[33m\"\u001b[39m\u001b[33mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\Ai_Exercises\\intro-to-tensorflow\\tf_intro_venv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[39m\n\u001b[32m     86\u001b[39m     sys.setdlopenflags(_default_dlopen_flags)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     89\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback.format_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     90\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     91\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     92\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     93\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mIf you need help, create an issue \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     94\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     95\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mand include the entire stack trace above this error message.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Traceback (most recent call last):\n  File \"c:\\Users\\User\\Desktop\\Ai_Exercises\\intro-to-tensorflow\\tf_intro_venv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TensorFlow Constant\n",
    "a = tf.constant(5.0)  #todo: create constant\n",
    "b = tf.constant(3.0)  \n",
    "\n",
    "# TensorFlow Variable\n",
    "x = tf.Variable(2.0) #todo: create a variable\n",
    "\n",
    "# Perform operations using constants and variables\n",
    "result = a * x + b\n",
    "\n",
    "# Print the result\n",
    "print(\"Result of a * x + b:\", result.numpy())\n",
    "\n",
    "# Update the variable\n",
    "x.assign(4.0) #todo: update x variable\n",
    "\n",
    "# Perform the operation again with updated variable\n",
    "new_result = a * x + b\n",
    "print(\"New result after updating x:\", new_result.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = .... #todo: define a tf tensor\n",
    "b = ....  # Define tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Placeholders (tf.placeholder):**\n",
    "These were used as inputs to the graph where the values would be provided at runtime via feed_dict. \n",
    "\n",
    "They allowed dynamic input feeding during the execution of the graph.\n",
    "\n",
    "**Session:** In TensorFlow 1.x, you needed to create a tf.Session() to run operations, and placeholders required you to feed values into them during session execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 1.x (use in legacy code or TensorFlow 1.x environments)\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.... #todo: disable v2 behavior so tf can act as v1\n",
    "tf.... #todo: disable eager execution so we can use session to test the behavior of v1\n",
    "\n",
    "# Placeholder for input values\n",
    "x = tf.placeholder(dtype=tf.float32, shape=None)\n",
    "\n",
    "# Constant\n",
    "a = ... #todo: create a constant\n",
    "b =  ... #todo: create a constant\n",
    "\n",
    "# Define an operation\n",
    "result = a * x + b\n",
    "\n",
    "# Create a session to run the computation graph\n",
    "with tf.Session() as sess:\n",
    "    # Feed a value into the placeholder and execute the graph\n",
    "    result_value = ... #todo: change the variable number through the session\n",
    "    print(\"Result with placeholder:\", result_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow 2.x:** Focuses on constants (tf.constant) and variables (tf.Variable). \n",
    "\n",
    "Eager execution removes the need for placeholders or sessions.\n",
    "\n",
    "**TensorFlow 1.x:** Used placeholders (tf.placeholder) for feeding data into graphs at runtime and required sessions to execute the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow: Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sum_result = tf... # Addition\n",
    "diff_result = tf.... # Subtraction\n",
    "quot_result = tf.... # Division\n",
    "prod_result = tf.... # Multiplication\n",
    "\n",
    "print(\"Sum of Tensors\",sum_result)\n",
    "print(\"Difference of tensors\",diff_result)\n",
    "print(\"Quotient of Tensor\",quot_result)\n",
    "print(\"Product of tensors \",prod_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise operations\n",
    "min_result = tf...(a, b)\n",
    "max_result = tf...(a, b)\n",
    "abs_result = tf...(a)\n",
    "\n",
    "print(\"Minimum:\", min_result)\n",
    "print(\"Maximum:\", max_result)\n",
    "print(\"Absolute value:\", abs_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the tensor `a` to float32 to match the type of 1e-8\n",
    "a = tf...(a, dtype=tf.float32)\n",
    "\n",
    "# Now you can safely compute the logarithm\n",
    "log_result = tf.math...(tf....(a, 1e-8))  # Ensure positive values for logarithm\n",
    "exp_result = tf...(a)\n",
    "\n",
    "\n",
    "print(\"Logarithm:\", log_result)\n",
    "print(\"Exponential:\", exp_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf....([3, 3, 3]) #todo: create a tensor\n",
    "\n",
    "scalar = tf....(2) #todo: create a constant\n",
    "broadcast_result = tf....(a, scalar)\n",
    "print(\"Broadcasted Addition:\", broadcast_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_axis = tf.reduce_sum(a, axis=0)  # Sum across the first axis\n",
    "mean_axis = tf.reduce_mean(a, axis=0)  # Mean across the second axis\n",
    "print(\"Sum across axis 0:\", sum_axis)\n",
    "print(\"Mean across axis 0:\", mean_axis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matmul example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Example: Input data for a neural network layer\n",
    "#todo: create input data tensor of shape (2,2), same for weights\n",
    "input_data = ....   # Shape: (2, 2)\n",
    "weights = ...    # Shape: (2, 2)\n",
    "\n",
    "# Matrix multiplication between input and weights\n",
    "output = tf..(input_data, weights)\n",
    "print(\"Neural Network Layer Output:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiply Example (Element Wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Example: Simulating image pixel values\n",
    "image = tf.constant([[255, 128], [64, 32]], dtype=tf.float32)  # Shape: (2, 2)\n",
    "\n",
    "# Scaling factor for each pixel (element-wise multiplication)\n",
    "#for example, you want to minimize the size of the image by halfw\n",
    "scaling_factor = 0.5\n",
    "scaled_image = tf.multiply(image, scaling_factor)\n",
    "print(\"Scaled Image:\\n\", scaled_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiply Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Example: Simulating a sequence of words (embeddings)\n",
    "sequence = ....  # Shape: (2, 3)\n",
    "\n",
    "# Attention scores (one for each word in the sequence)\n",
    "attention_scores = ....  #Shape: (2, 3)\n",
    "\n",
    "# Apply attention weights using element-wise multiplication\n",
    "weighted_sequence = tf....(sequence, attention_scores)\n",
    "print(\"Weighted Sequence:\\n\", weighted_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Derivates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(1.0)\n",
    "\n",
    "def f(x):\n",
    "  y = x**2 + 2*x - 5\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf......() as tape: #use GradientTape to calculate gradients\n",
    "  y = f(x)\n",
    "\n",
    "g_x = tape.gradient(y, x)  # g(x) = dy/dx\n",
    "\n",
    "g_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow: Tensor vs Numpy Ndarray\n",
    "Let's do some benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create large random matrices\n",
    "size = 10000\n",
    "np_matrix_a = np.random.randn(size, size)\n",
    "np_matrix_b = np.random.randn(size, size)\n",
    "tf_matrix_a = tf.random.normal((size, size))\n",
    "tf_matrix_b = tf.random.normal((size, size))\n",
    "\n",
    "# Benchmark NumPy\n",
    "start_time = time.time()\n",
    "np_result = np.dot(np_matrix_a, np_matrix_b)\n",
    "np_duration = time.time() - start_time\n",
    "\n",
    "# Benchmark TensorFlow on CPU\n",
    "start_time = time.time()\n",
    "tf_result = tf.matmul(tf_matrix_a, tf_matrix_b)\n",
    "tf_duration = time.time() - start_time\n",
    "\n",
    "print(\"NumPy Duration: {:.6f} seconds\".format(np_duration))\n",
    "print(\"TensorFlow (CPU) Duration: {:.6f} seconds\".format(tf_duration))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example forward propagation in Python\n",
    "def forward_propagation(X, W, b):\n",
    "    Z = np.dot(W, X) + b  # Weighted sum\n",
    "    A = 1 / (1 + np.exp(-Z))  # Sigmoid activation function\n",
    "    return A\n",
    "\n",
    "# Dummy data\n",
    "X = np.array([[1.0], [2.0], [3.0]])  # Input\n",
    "W = np.array([[0.2, 0.4, 0.6]])  # Weights\n",
    "b = np.array([[0.5]])  # Bias\n",
    "\n",
    "# Forward propagation\n",
    "output = forward_propagation(X, W, b)\n",
    "print(\"Output of forward propagation:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return .....\n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np....(0, x) \n",
    "\n",
    "# Tanh activation function\n",
    "def tanh(x):\n",
    "    return np.....(x)\n",
    "\n",
    "# Example of activation functions\n",
    "x = np.array([-1.0, 0.0, 1.0, 2.0])\n",
    "print(\"Sigmoid:\", sigmoid(x))\n",
    "print(\"ReLU:\", relu(x))\n",
    "print(\"Tanh:\", tanh(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_with_activation(X, W, b, activation_function):\n",
    "    Z = np.dot(W, X) + b  # Weighted sum\n",
    "    match activation_function:\n",
    "        case 'sigmoid': \n",
    "            A = sigmoid(Z)\n",
    "        case 'relu' : \n",
    "            A = relu(Z)\n",
    "        case 'tanh' :\n",
    "            A = tanh(Z)\n",
    "    return A\n",
    "\n",
    "# Testing the function\n",
    "W = np.array([[0.2, 0.4, 0.6]])\n",
    "b = np.array([[0.5]])\n",
    "X = np.array([[1.0], [2.0], [3.0]])\n",
    "\n",
    "output = forward_with_activation(X, W, b, activation_function='relu')\n",
    "print(\"Output with ReLU activation:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_intro_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
